# Personal AI Chat Manager - Complete System Blueprint ## 🎯 System Overview A local-first, single-user AI conversation management system that ingests, analyzes, correlates, and provides intelligent access to conversations across multiple AI platforms. Built for privacy, performance, and deep personal knowledge synthesis. ### Core Value Proposition - **Privacy-First**: All processing happens locally on your hardware - **Cross-Platform Intelligence**: Unify conversations from Claude, ChatGPT, Ollama, etc. - **Semantic Understanding**: Local AI models extract insights and connections - **Personal Knowledge Base**: Auto-generated wiki articles from conversation synthesis - **High Performance**: Optimized for single-user scale with RTX 3090/3080 Ti + 64GB RAM --- ## 🏗️ System Architecture ### Layer 1: Data Ingestion & Normalization #### **Supported Import Formats** - **ZIP/JSONL/NDJSON**: Batch conversation exports - **Raw JSON/HTML**: Platform-specific export formats - **Common Chat Formats**: ChatGPT JSON, Claude conversations, custom formats - **Attachments**: Code files, images, documents embedded in conversations #### **Normalization Pipeline** ``` Raw Imports → Format Detection → Checksum Deduplication → Content Extraction → Canonical Transform ``` **Key Features:** - **Checksum Deduplication**: SHA-256 hashing to eliminate duplicate imports - **MIME Detection**: Automatic content type identification for attachments - **Attachment Extraction**: Code snippets, images, files separated and indexed - **Timestamp Normalization**: UTC standardization across all platforms - **Identity Mapping**: Collapse platform-specific aliases to single user profile #### **Processing Components** - **Format Parsers**: Modular parsers for each platform export format - **Content Sanitizer**: Clean and validate imported data - **Attachment Processor**: Extract, classify, and store file attachments - **Metadata Enricher**: Add import timestamps, source attribution, quality scores --- ### Layer 2: Canonical Data Model #### **Core Entities** **Platform** ```sql - id: UUID - name: String (Claude, ChatGPT, Ollama, etc.) - api_version: String - export_format_version: String - created_at, updated_at: Timestamp ``` **Thread** ```sql - id: UUID - platform_id: UUID (FK) - external_id: String (platform's thread ID) - title: String (auto-generated or extracted) - summary: Text (AI-generated) - created_at, updated_at: Timestamp - message_count: Integer - total_tokens: Integer (estimated) - quality_score: Float (0-1) ``` **Message** ```sql - id: UUID - thread_id: UUID (FK) - role: Enum (user, assistant, system) - content: Text - content_type: Enum (text, code, image, file) - timestamp: Timestamp - sequence_number: Integer - token_count: Integer (estimated) - has_attachments: Boolean ``` **Attachment** ```sql - id: UUID - message_id: UUID (FK) - filename: String - mime_type: String - file_size: Integer - content_hash: String (SHA-256) - storage_path: String - extracted_text: Text (for searchable content) - metadata: JSONB (language, framework, etc.) ``` **Entity** ```sql - id: UUID - name: String (canonical name) - type: Enum (person, technology, concept, project, etc.) - aliases: String[] (alternative names) - description: Text - confidence: Float (0-1) - first_mentioned: Timestamp - mention_count: Integer ``` **Topic** ```sql - id: UUID - name: String - description: Text - keywords: String[] - parent_topic_id: UUID (FK, optional) - confidence: Float (0-1) - thread_count: Integer - auto_generated: Boolean ``` **Tag** ```sql - id: UUID - name: String - color: String (hex) - description: Text - auto_generated: Boolean - usage_count: Integer ``` **Embedding** ```sql - id: UUID - content_id: UUID (can reference message, thread, attachment) - content_type: Enum (message, thread_summary, attachment) - model_name: String (BGE-M3, etc.) - vector: Vector (dimensions depend on model) - created_at: Timestamp ``` **LinkEdge** ```sql - id: UUID - source_thread_id: UUID (FK) - target_thread_id: UUID (FK) - link_type: Enum (continuation, related, contradiction, solution) - score: Float (0-1) - evidence_bits: Integer (bit flags for evidence types) - policy_version: String - created_at: Timestamp - validated_by_human: Boolean - human_override: Boolean ``` **Audit** ```sql - id: UUID - table_name: String - record_id: UUID - action: Enum (insert, update, delete) - old_values: JSONB - new_values: JSONB - changed_by: String (system, user, model_name) - timestamp: Timestamp ``` #### **Database Design** - **Primary DB**: PostgreSQL 15+ with pgvector extension - **Caching**: Redis for frequently accessed data and search results - **File Storage**: Local filesystem with organized directory structure - **Indexes**: Optimized for single-user patterns - FTS GIN indexes on message content, thread summaries - HNSW vector indexes for semantic search - B-tree indexes on timestamps, platform_id, entity references - **Partitioning**: Time-based partitioning for messages (optional, based on volume) --- ### Layer 3: Analysis & Modeling (Local-First) #### **Local AI Infrastructure** **Primary Models:** - **Embedding**: BGE-M3 (multilingual, code-aware) - **Reranker**: bge-reranker-v2-m3 (cross-encoder for relevance) - **Chat/Analysis**: Llama 3.1 70B or Mixtral 8x7B (via Ollama) - **Code Analysis**: CodeLlama 34B (specialized tasks) **Model Serving:** - **Ollama**: Primary interface for chat models - **vLLM**: Alternative for high-throughput scenarios - **llama.cpp**: Lightweight option for resource-constrained analysis #### **Analysis Pipeline** **Content Summarization** - Thread-level summaries (key topics, outcomes, unresolved questions) - Long attachment summarization (code explanations, document summaries) - Multi-turn conversation flow analysis **Topic & Intent Classification** - Custom label sets trained on your conversation patterns - Hierarchical topic classification (Programming → Python → FastAPI) - Intent detection (learning, troubleshooting, brainstorming, implementation) - Conversation outcome classification (solved, pending, ongoing, abandoned) **Entity Extraction & Resolution** - Named entity recognition (people, technologies, projects, concepts) - Entity linking and alias resolution - Relationship extraction between entities - Temporal entity tracking (when first mentioned, evolution over time) **Embedding Generation** - Message-level embeddings for semantic search - Thread-level embeddings for conversation similarity - Code snippet embeddings for technical search - Attachment embeddings for document retrieval #### **Analysis Workflows** ``` New Content → Entity Extraction → Topic Classification → Embedding Generation → Summary Creation ↓ Existing Content ← Entity Resolution ← Topic Hierarchy ← Similarity Analysis ← Quality Scoring ``` --- ### Layer 4: Correlation & Pairing #### **Candidate Generation** **Semantic KNN** - Vector similarity search with configurable thresholds - Constrained by temporal windows (conversations within N days) - Platform-aware rules (same platform vs. cross-platform linking) - Content type filtering (code-to-code, discussion-to-discussion) **Temporal Constraints** - Sliding time windows for recent correlation - Decay functions for relevance over time - Burst detection for intensive topic exploration periods - Cross-session continuation detection #### **Evidence Fusion** **Scoring Components:** - **Semantic Score**: Cosine similarity between embeddings (0.0-1.0) - **Temporal Proximity**: Time-based relevance decay (0.0-1.0) - **Entity Overlap**: Shared entities with importance weighting (0.0-1.0) - **Topic Coherence**: Topic classification alignment (0.0-1.0) - **Code Similarity**: AST-based code comparison for technical content (0.0-1.0) - **User Pattern**: Historical correlation validation (0.0-1.0) **Evidence Bit Flags:** ``` 0x01: High semantic similarity (>0.8) 0x02: Temporal proximity (<24 hours) 0x04: Entity overlap (>2 shared entities) 0x08: Topic coherence (same primary topic) 0x10: Code similarity (>0.7) 0x20: User validation (manually confirmed) 0x40: Pattern match (fits historical correlation patterns) 0x80: Reserved for future evidence types ``` **Threshold Management:** - Configurable score thresholds per link type - Policy versioning for threshold updates - A/B testing framework for threshold optimization #### **Re-weaving Process** **Trigger Conditions:** - Model updates (new embedding model, improved classification) - Policy changes (threshold adjustments, new evidence types) - Manual request (user-initiated full re-analysis) - Scheduled maintenance (weekly/monthly full correlation refresh) **Process:** 1. **Backup Current Links**: Preserve existing correlations 2. **Regenerate Embeddings**: If model changed 3. **Recalculate Scores**: Apply new policies/thresholds 4. **Diff Analysis**: Compare old vs. new link sets 5. **Conflict Resolution**: Handle disagreements between old/new 6. **Human Review Queue**: Flag ambiguous changes for review 7. **Commit Changes**: Update LinkEdge table with new policy_version #### **Human-in-the-Loop** **Adjudication Queue:** - Low-confidence links (score near threshold) - Conflicting evidence (high semantic, low temporal) - User-flagged correlations for review - Model disagreements during re-weaving **Review Interface:** - Side-by-side conversation display - Evidence breakdown with confidence scores - Quick approve/reject/modify actions - Feedback incorporation for model improvement --- ### Layer 5: Hybrid Search & Retrieval #### **Search Architecture** ``` Query → Query Analysis → Parallel Search → Result Fusion → Ranking → Response ↓ ↓ ↓ ↓ ↓ Intent/Entity → [FTS + Vector] → Score Fusion → Re-ranking → Snippets ``` #### **Search Components** **Full-Text Search (PostgreSQL FTS)** - GIN indexes on message content, thread summaries, attachment text - Custom dictionaries for technical terms - Phrase search, proximity search, boolean operators - Platform-specific search (e.g., "FastAPI site:claude") **Vector Search (pgvector)** - HNSW indexes for approximate nearest neighbor - Configurable distance metrics (cosine, L2, inner product) - Multi-vector search (content + code + summary vectors) - Temporal vector weighting (recent conversations ranked higher) **Hybrid Fusion** - **Static Blending**: Weighted combination (e.g., 0.6 * semantic + 0.4 * keyword) - **Learned Blending**: ML model to optimize weights based on query type - **Dynamic Blending**: Query analysis determines optimal fusion strategy #### **Advanced Filtering** **Faceted Search:** - Platform (Claude, ChatGPT, Ollama, etc.) - Date ranges (last week, month, year, custom) - Entities (people, technologies, projects mentioned) - Topics (hierarchical topic tree) - Tags (user-defined and auto-generated) - Attachment types (code, images, documents) - Quality scores (high-quality conversations only) - Resolution status (solved, pending, ongoing) **Query Enhancement:** - Auto-completion based on previous searches and content - Query expansion using entity aliases and synonyms - Typo correction and fuzzy matching - Natural language query parsing #### **Result Explanation** **Match Evidence:** - Keyword highlighting in content - Semantic similarity scores - Relevant entity matches - Topic classification confidence - Time-based relevance factors **Snippet Generation:** - Context-aware excerpt extraction - Multi-modal snippets (text + code + images) - Conversation flow context (previous/next messages) - Cross-reference links to related conversations #### **Saved Searches & Alerts** - **Search Persistence**: Save complex queries with filters - **Smart Folders**: Dynamic collections based on search criteria - **Local Alerts**: System notifications for new content matching saved searches - **Webhook Integration**: Optional external notifications --- ### Layer 6: Web Application (UX) #### **Global Search Interface** **Search Bar:** - Prominent, always-accessible search - Real-time suggestions and auto-completion - Query history and favorites - Advanced search builder (visual filter construction) **Facet Rail:** - Collapsible sidebar with all filter categories - Dynamic facet counts based on current results - One-click filter application/removal - Filter state persistence across sessions **Results Display:** - Card-based layout with conversation previews - Relevance badges (semantic, keyword, temporal) - Quick actions (save, tag, link, export) - Infinite scroll with performance optimization #### **Thread Reader** **Main Content:** - Clean, readable conversation flow - Message timestamps and platform indicators - Inline code syntax highlighting - Image/attachment inline display - Message-level actions (copy, reference, annotate) **Sidebar Features:** - Auto-generated thread summary - Extracted entities with click-through - Related conversations (auto-discovered links) - Topic classifications and confidence scores - Manual tagging and note-taking - Export options for individual threads #### **Conversation Pairing View** **Side-by-Side Layout:** - Synchronized scrolling between related conversations - Evidence toggle (show/hide correlation reasoning) - Link strength visualization - Quick navigation between linked sections **Evidence Panel:** - Detailed breakdown of correlation scores - Entity overlap highlighting - Temporal relationship timeline - User validation controls (confirm/reject/modify) #### **Knowledge Graph Visualization** **Interactive Graph:** - Nodes: conversations, topics, entities - Edges: correlations, relationships, temporal connections - Filterable by platform, time range, topic, quality - Zoom and pan with performance optimization - Click-through navigation to conversations **Graph Controls:** - Layout algorithms (force-directed, hierarchical, circular) - Node sizing (by importance, recency, quality) - Edge thickness (by correlation strength) - Color coding (by platform, topic, time period) #### **Timeline Views** **Platform Timeline:** - Horizontal timeline with conversation markers - Platform-specific lanes for comparison - Zoom/brush for date range selection - Activity density visualization **Topic Evolution:** - Topic-specific timeline showing knowledge development - Branching visualization for related subtopics - Quality progression over time - Key insight markers #### **Export & Sharing** **Export Formats:** - JSONL: Raw data for programmatic access - CSV: Tabular data for spreadsheet analysis - ZIP: Complete archive with attachments - HTML: Standalone readable format - Markdown: Wiki-style documentation **Privacy Controls:** - Content redaction for sensitive information - Selective export (choose specific threads/topics) - Anonymization options for external sharing #### **Accessibility & Responsive Design** - **WCAG 2.1 AA Compliance**: Screen reader support, keyboard navigation - **Dark/Light Mode**: User preference with system detection - **Responsive Layout**: Mobile-friendly design for search and reading - **Performance**: Fast loading with lazy-loading for large result sets --- ### Layer 7: API (Local-Only by Default) #### **API Design Principles** - **REST/JSON**: Standard HTTP methods and JSON responses - **OpenAPI 3.0**: Complete API documentation with examples - **Versioned Routes**: `/v1` prefix for future compatibility - **Consistent Errors**: Standardized error format across all endpoints #### **Core Endpoints** **Search & Discovery** ``` GET /v1/search - q: query string - filters: JSON object with facet filters - limit/offset: pagination - sort: relevance, date, quality - explain: include relevance scoring details GET /v1/search/suggest - q: partial query for auto-completion - type: query, entity, topic suggestions POST /v1/search/saved - Save search query with filters GET /v1/search/saved - List user's saved searches ``` **Content Access** ``` GET /v1/threads - filters, pagination, sorting GET /v1/threads/{id} - Full thread with messages and metadata GET /v1/threads/{id}/related - Linked conversations with correlation scores GET /v1/messages/{id} - Individual message with context GET /v1/attachments/{id} - Attachment metadata and download link ``` **Entity & Topic Management** ``` GET /v1/entities - List entities with filtering GET /v1/entities/{id} - Entity details with related content POST /v1/entities/{id}/merge - Merge duplicate entities GET /v1/topics - Topic hierarchy with statistics POST /v1/topics - Create custom topic PUT /v1/topics/{id} - Update topic metadata ``` **Link Management** ``` GET /v1/links - List correlations with filtering POST /v1/links - Create manual link between threads PUT /v1/links/{id} - Update link score or validation DELETE /v1/links/{id} - Remove correlation link POST /v1/links/reweave - Trigger correlation re-analysis GET /v1/links/queue - Human review queue ``` **Import & Export** ``` POST /v1/import - Upload conversation data (multipart/form-data) GET /v1/import/jobs - List import job status GET /v1/import/jobs/{id} - Import job details and progress POST /v1/export - Request data export with options GET /v1/export/jobs/{id} - Export job status and download ``` **System Management** ``` GET /v1/health - Basic health check GET /v1/ready - Readiness check (DB, Redis, models) GET /v1/stats - System statistics and metrics POST /v1/maintenance/reindex - Trigger search index rebuild ``` #### **Security & Access Control** **Default Configuration:** - Bind to `127.0.0.1` only (localhost access) - No authentication required for local access - CORS disabled by default **Optional Network Access:** - Single master API key for LAN access - IP whitelist for trusted devices - Rate limiting to prevent abuse - Request/response logging for audit #### **Pagination & Performance** - **Consistent Pagination**: `limit`/`offset` with `total_count` - **Performance Limits**: Max 100 items per request - **Async Processing**: Long-running operations (import, export, reweave) use job queue - **Caching**: Redis cache for frequently accessed data --- ### Layer 8: Observability & Reliability #### **Health Monitoring** **Endpoint Health Checks:** ``` GET /health - Basic application health - Response time: <100ms - Returns: {"status": "healthy", "timestamp": "..."} GET /ready - Deep readiness check - Database connectivity - Redis availability - Model loading status - Disk space availability - Response time: <500ms ``` **Component Probes:** - **Database**: Connection pool health, query performance - **Redis**: Connection status, memory usage - **Models**: Loading status, inference availability - **Storage**: Disk space, file system health #### **Metrics & Logging** **Structured JSON Logging:** ```json { "timestamp": "2024-11-16T15:30:45Z", "level": "INFO", "component": "search", "operation": "hybrid_search", "duration_ms": 156, "query": "FastAPI performance", "results_count": 23, "user_id": "local_user" } ``` **Key Metrics:** - **Ingestion Rate**: messages/sec, threads/sec - **Search Performance**: p50, p95, p99 response times - **Model Performance**: inference time, queue depth - **Error Rates**: by component, endpoint, model - **Resource Usage**: CPU, memory, GPU utilization **Metrics Collection:** - In-memory metrics with periodic aggregation - Optional Prometheus export for advanced monitoring - Basic dashboard in web UI - CLI tools for metric queries #### **Reliability Features** **Smoke Testing:** ```bash # Automated smoke test script smoke_test.sh: - UI accessibility check - API endpoint validation - Database query test - Model inference test - Search functionality test - Import/export test ``` **Backup Strategy:** - **Nightly Logical Dumps**: Full PostgreSQL backup - **WAL Archiving**: Point-in-time recovery capability - **Config Backup**: Environment and model configs - **Attachment Backup**: File system sync to backup location **Recovery Procedures:** - Database restoration from logical dumps - Point-in-time recovery from WAL archives - Model re-download and cache rebuild - Import data validation and retry mechanisms --- ### Layer 9: Performance & Scale (Single User) #### **Database Optimization** **PostgreSQL Tuning:** ```sql -- Optimized for single-user workload shared_buffers = 8GB # 1/4 of RAM effective_cache_size = 48GB # 3/4 of RAM work_mem = 256MB # For complex queries maintenance_work_mem = 2GB # For index building wal_buffers = 64MB # Write-ahead logging checkpoint_completion_target = 0.9 # Smooth checkpoints random_page_cost = 1.1 # SSD-optimized ``` **Vector Index Tuning (HNSW):** - **ef_construction**: 200 (build quality vs. speed) - **M**: 16 (connections per node) - **ef_search**: 100 (search quality vs. speed) - **Maintenance**: Periodic REINDEX for optimal performance **Full-Text Search (GIN):** - **fastupdate**: off (better for bulk operations) - **gin_pending_list_limit**: 16MB - **Custom dictionaries**: Technical terms, abbreviations #### **Model Performance** **GPU Optimization (RTX 3090/3080 Ti):** - **Batch Sizing**: Tuned for GPU memory (24GB/12GB) - **Model Quantization**: 4-bit/8-bit for memory efficiency - **Concurrent Inference**: Multiple model instances for parallel processing - **Memory Management**: Model swapping for resource optimization **Embedding Generation:** ```python # Optimized batch processing batch_size = 32 if gpu_memory >= 20GB else 16 max_concurrent_models = 2 embedding_cache_size = 10000 # LRU cache for repeated content ``` **Analysis Pipeline:** - **Worker Concurrency**: 4-8 workers based on CPU cores - **Queue Management**: Redis-based job queue with priority - **Incremental Processing**: Only analyze new/changed content - **Resource Throttling**: Dynamic batch sizing based on system load #### **Caching Strategy** **Multi-Level Caching:** 1. **Application Cache**: In-memory LRU for hot data 2. **Redis Cache**: Shared cache for search results, embeddings 3. **Database Cache**: PostgreSQL buffer cache optimization 4. **File System Cache**: OS-level caching for attachments **Cache Policies:** - **Search Results**: 1 hour TTL with invalidation on new content - **Embeddings**: Persistent cache with model version tracking - **Thread Summaries**: 24 hour TTL with manual refresh option - **Entity Mappings**: Long-lived cache with incremental updates #### **Storage Optimization** **File Organization:** ``` data/ ├── attachments/ │ ├── yyyy/mm/dd/ # Date-based partitioning │ │ ├── images/ │ │ ├── code/ │ │ └── documents/ ├── exports/ ├── backups/ └── temp/ ``` **Compression & Deduplication:** - **Attachment deduplication**: SHA-256 hash-based storage - **Text compression**: gzip for large text content - **Image optimization**: WebP conversion for web display - **Archive management**: Automated cleanup of old temporary files #### **Scale Thresholds** **Single-Instance Limits:** - **Conversations**: 100,000+ threads - **Messages**: 10,000,000+ individual messages - **Embeddings**: 50,000,000+ vectors (depending on dimensions) - **Search Performance**: Sub-second response for 95% of queries - **Concurrent Users**: 1 (local), 5-10 (LAN access) **Scale-Out Options (if needed):** - **Elasticsearch**: For advanced text search at scale - **Neo4j**: For complex graph relationships - **Distributed Embeddings**: Separate vector database (Qdrant, Weaviate) - **Horizontal Sharding**: Time-based or topic-based data partitioning --- ### Layer 10: Configuration & Operations #### **Environment Configuration** **Core `.env` File:** ```bash # Database DATABASE_URL=postgresql://user:pass@localhost:5432/ai_chat_manager REDIS_URL=redis://localhost:6379/0 # Models EMBEDDING_MODEL=BAAI/bge-m3 RERANKER_MODEL=BAAI/bge-reranker-v2-m3 CHAT_MODEL=llama3.1:70b CODE_MODEL=codellama:34b # Thresholds CORRELATION_THRESHOLD=0.7 SEMANTIC_THRESHOLD=0.8 TEMPORAL_WINDOW_DAYS=30 # Performance MAX_BATCH_SIZE=32 WORKER_CONCURRENCY=6 CACHE_SIZE_MB=1024 # Security (optional) API_KEY=your-secure-api-key-here BIND_ADDRESS=127.0.0.1 CORS_ORIGINS= # Features ENABLE_WEBHOOKS=false ENABLE_REWEAVE_SCHEDULER=true BACKUP_ENABLED=true ``` #### **Admin CLI Tool** ```bash # Database management ai-chat-manager db migrate # Apply database migrations ai-chat-manager db backup # Create backup ai-chat-manager db restore latest # Restore from backup # Content management ai-chat-manager import --format=chatgpt exports/chatgpt.zip ai-chat-manager export --format=jsonl --output=/path/to/export.jsonl ai-chat-manager reindex search # Rebuild search indexes ai-chat-manager reindex vectors # Rebuild vector indexes # Correlation management ai-chat-manager reweave --policy=latest # Full correlation refresh ai-chat-manager reweave --incremental # Process new content only ai-chat-manager links validate # Validate existing links # Model management ai-chat-manager models list # Show loaded models ai-chat-manager models reload embedding # Reload specific model ai-chat-manager models benchmark # Performance testing # Maintenance ai-chat-manager vacuum # Database cleanup ai-chat-manager prune --older-than=30d # Remove old temporary files ai-chat-manager health --verbose # Detailed health check ``` #### **Model Registry** **`models.yaml` Configuration:** ```yaml embedding: primary: name: "BAAI/bge-m3" dimensions: 1024 max_tokens: 8192 languages: ["en", "zh", "es", "fr", "de"] gpu_memory_mb: 2048 fallback: name: "sentence-transformers/all-MiniLM-L6-v2" dimensions: 384 max_tokens: 512 reranker: name: "BAAI/bge-reranker-v2-m3" max_pairs: 100 gpu_memory_mb: 1024 chat: primary: name: "llama3.1:70b" context_length: 32768 gpu_memory_mb: 45000 quantization: "q4_k_m" fallback: name: "llama3.1:8b" context_length: 32768 gpu_memory_mb: 8000 limits: max_concurrent_embeddings: 4 max_concurrent_chat: 2 embedding_batch_size: 32 chat_timeout_seconds: 300 ``` #### **Migration System** **Safe Database Migrations:** - **Version Tracking**: Sequential migration numbering - **Rollback Support**: Down migrations for all changes - **Two-Phase Deployment**: Schema changes followed by data migrations - **Validation**: Pre and post-migration data integrity checks **Migration Example:** ```sql -- Migration 003: Add correlation evidence tracking -- Up migration ALTER TABLE link_edges ADD COLUMN evidence_bits INTEGER DEFAULT 0; ALTER TABLE link_edges ADD COLUMN policy_version VARCHAR(20) DEFAULT '1.0'; CREATE INDEX idx_link_edges_evidence ON link_edges(evidence_bits); -- Down migration ALTER TABLE link_edges DROP COLUMN evidence_bits; ALTER TABLE link_edges DROP COLUMN policy_version; DROP INDEX idx_link_edges_evidence; ``` --- ### Layer 11: Import/Export & Interoperability #### **Import Pipeline** **Drag-and-Drop Interface:** - Browser-based file upload with progress tracking - Multiple file selection with format auto-detection - Real-time import status and error reporting - Resumable uploads for large files **API Import:** ```bash # Batch import curl -X POST \ -F "files=@chatgpt-export.zip" \ -F "files=@claude-conversations.jsonl" \ -F "platform=auto-detect" \ http://localhost:8000/v1/import # Single conversation curl -X POST \ -H "Content-Type: application/json" \ -d @conversation.json \ http://localhost:8000/v1/import/conversation ``` **Import Job Management:** - **Queued Processing**: Background job queue for large imports - **Progress Tracking**: Real-time status updates via WebSocket - **Error Handling**: Detailed error reports with retry options - **Validation**: Content validation before database insertion - **Rollback**: Ability to undo failed imports #### **Export System** **Deterministic Exports:** ```json { "export_metadata": { "version": "1.0", "created_at": "2024-11-16T15:30:45Z", "total_threads": 1247, "total_messages": 25891, "checksum": "sha256:abc123..." }, "schema_version": "1.2.0", "content": { "threads": [...], "messages": [...], "entities": [...], "links": [...] } } ``` **Export Options:** - **Full Export**: Complete database dump with all relationships - **Filtered Export**: By date range, platform, topic, quality - **Redacted Export**: Remove sensitive information for sharing - **Incremental Export**: Only changes since last export - **Format Options**: JSON, JSONL, CSV, ZIP archive **Privacy Controls:** ```yaml redaction_rules: - type: "email" action: "mask" # user@domain.com → u***@domain.com - type: "api_key" action: "remove" # Complete removal - type: "personal_name" action: "pseudonym" # Consistent fake names - type: "code_comment" pattern: "# TODO.*personal.*" action: "remove" ``` #### **Interoperability Standards** **Platform Format Support:** - **ChatGPT**: Official JSON export format - **Claude**: Anthropic conversation format - **Bard/Gemini**: Google export format - **Character.AI**: Chat export format - **Custom**: Extensible parser framework **Data Exchange Formats:** - **JSONL**: Streaming format for large datasets - **CSV**: Tabular format for analysis tools - **XML**: Structured format for specialized tools - **HTML**: Human-readable archived conversations - **Markdown**: Documentation-friendly format **API Compatibility:** - **OpenAI-compatible**: Search API mimics OpenAI format - **Webhook Standards**: Standard HTTP webhook format - **RSS/Atom**: Feed format for new conversation notifications - **OPML**: Outline format for topic hierarchies --- ## 🚀 Implementation Roadmap ### Phase 1: Foundation (Weeks 1-4) 1. **Database Setup**: PostgreSQL + pgvector + Redis 2. **Core Data Model**: Implement all entity tables and relationships 3. **Import Pipeline**: Basic format parsers for ChatGPT and Claude 4. **Local AI Setup**: Ollama installation and model configuration 5. **Basic Web UI**: Simple search and conversation viewing ### Phase 2: Analysis (Weeks 5-8) 1. **Embedding Generation**: BGE-M3 integration and batch processing 2. **Entity Extraction**: Local NER and entity resolution 3. **Topic Classification**: Custom topic modeling pipeline 4. **Basic Correlation**: Semantic similarity with simple thresholds 5. **Search Implementation**: FTS + vector search with basic fusion ### Phase 3: Intelligence (Weeks 9-12) 1. **Advanced Correlation**: Multi-evidence fusion with re-weaving 2. **Human-in-the-Loop**: Review interface for ambiguous links 3. **Wiki Generation**: Auto-article creation from conversation synthesis 4. **Advanced Search**: Faceted search with complex filtering 5. **Performance Optimization**: Caching, indexing, batch processing ### Phase 4: Polish (Weeks 13-16) 1. **UI/UX Refinement**: Advanced visualizations and responsive design 2. **API Completion**: Full REST API with OpenAPI documentation 3. **Export System**: Multiple formats with privacy controls 4. **Monitoring**: Health checks, metrics, and logging 5. **Documentation**: Complete user and admin documentation ### Phase 5: Scale & Extend (Weeks 17-20) 1. **Performance Tuning**: Database optimization and caching strategies 2. **Advanced Features**: Saved searches, alerts, webhook integration 3. **Model Management**: Model registry and hot-swapping 4. **Backup & Recovery**: Automated backup and restoration procedures 5. **Testing & Validation**: Comprehensive test suite and performance benchmarks --- ## 📊 Success Metrics ### Technical Performance - **Import Speed**: 1000+ messages/minute processing rate - **Search Latency**: <200ms for 95% of search queries - **Correlation Accuracy**: >85% user validation rate for auto-generated links - **Storage Efficiency**: <50MB per 1000 messages including embeddings - **Uptime**: >99.9% availability for local access ### User Experience - **Search Relevance**: >90% of searches return useful results in top 10 - **Knowledge Discovery**: Users discover 3+ relevant old conversations per week - **Time Savings**: 50% reduction in time spent searching for previous solutions - **Content Quality**: Auto-generated wiki articles rated 4+/5 for usefulness ### System Reliability - **Data Integrity**: Zero data loss during imports and migrations - **Recovery Time**: <5 minutes to restore from backup - **Error Rate**: <0.1% of operations result in errors - **Resource Usage**: <80% of available GPU memory and disk space --- This blueprint provides a comprehensive foundation for building a sophisticated, local-first AI conversation management system optimized for single-user scenarios while maintaining the flexibility to scale as your corpus grows.